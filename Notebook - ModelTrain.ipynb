{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "504a32e6",
   "metadata": {},
   "source": [
    "### Entraînement d'un modèle GRU pour la prédiction du prix du Bitcoin\n",
    "- Ce Notebook entraîne un modèle GRU pour prédire le prix du BTC à court terme.\n",
    "- Il suit plusieurs étapes : chargement des données, préparation des indicateurs, configuration des hyperparamètres, entraînement et évaluation du modèle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b435f2d",
   "metadata": {},
   "source": [
    "### 1. Importation des données\n",
    "- Importation des bibliothèques nécessaires (Pandas, `tech_lib` pour les indicateurs techniques, `os` pour la gestion des fichiers, etc.).\n",
    "- Chargement du dataset du BTC en fréquence 1 minute sur une période définie.\n",
    "- Ajout de cibles (`Close_target` et `DirVar_target`) pour la prédiction.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Calcul des indicateurs techniques\n",
    "- Calcul du Chaikin Money Flow (`cmf`) avec une fenêtre de 60 minutes.\n",
    "- Calcul de la Volume Weighted Moving Average (`vwma`) avec des fenêtres de 60 et 120 minutes.\n",
    "- Calcul de l'On-Balance Volume (`obv`).\n",
    "- Suppression des valeurs `NaN` après le calcul des indicateurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tech_lib as tech\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import dataset\n",
    "ticker = 'BTCUSDT'\n",
    "freq = '1m'\n",
    "start_date_str = datetime(*(2024, 1, 15, 0, 0, 0)).strftime('%Y%m%d')\n",
    "end_date_str = datetime(*(2025, 1, 15, 0, 0, 0)).strftime('%Y%m%d')\n",
    "dataset = pd.read_csv(os.getcwd()+f'\\\\dataset\\\\\\{ticker}-{freq}-{start_date_str}-{end_date_str}.csv', index_col='Open Time')\n",
    "\n",
    "#Calcul des outputs\n",
    "dataset['Close_target'] = dataset['Close'].shift(-1)\n",
    "dataset['DirVar_target'] = pd.Series([1 if diff > 0 else 0 for diff in dataset['Close'].diff(1)], index=dataset.index, dtype=int).shift(-1)\n",
    "\n",
    "# Boucle pour calculer les indicateurs avec différentes fenêtres\n",
    "window = 60\n",
    "dataset[f'cmf_{window}'] = tech.cmf(\n",
    "    high=dataset.High, \n",
    "    low=dataset.Low, \n",
    "    close=dataset.Close, \n",
    "    volume=dataset.Volume, \n",
    "    window=window\n",
    ")\n",
    "\n",
    "for window in [60, 120]:\n",
    "    dataset[f'vwma_{window}'] = tech.vwma(\n",
    "        close=dataset.Close, \n",
    "        volume=dataset.Volume, \n",
    "        window=window\n",
    "    )\n",
    "    \n",
    "dataset['obv'] = tech.obv(dataset.Close, dataset.Volume)\n",
    "\n",
    "dataset = dataset.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Définition des hyperparamètres et sélection des entrées\n",
    "- Définition d'une grille de recherche pour les hyperparamètres (`hidden_size`, `num_layers`, `dropout`, `lr`, etc.).\n",
    "- Sélection des variables d'entrée pour le modèle, incluant les prix (`High`, `Low`, `Open`, `Close`), le volume et les indicateurs calculés.\n",
    "- Génération de combinaisons d'hyperparamètres et de variables d'entrée pour tester différentes configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "param_grid = {\n",
    "    \"period_size\"   : [10000],\n",
    "    \"hidden_size\"   : [8],\n",
    "    \"num_layers\"    : [2],\n",
    "    \"dropout\"       : [0.15],\n",
    "    \"lr\"            : [0.001],\n",
    "    \"batch_size\"    : [128],\n",
    "    \"epochs\"        : [3],\n",
    "    \"num_workers\"   : [4],\n",
    "    \"delta\"         : [1],\n",
    "    \"weight_decay\"  : [1e-4],\n",
    "    \"train_size\"    : [0.7],\n",
    "    \"val_size\"      : [0.15],\n",
    "    \"test_size\"     : [0.15],\n",
    "    \"seq_size\"      : [60],\n",
    "    \"device\"        : [\"cuda\"]\n",
    "}\n",
    "\n",
    "inputs = [\n",
    "    'High', 'Low', 'Open', 'Close',\n",
    "    'Volume', 'obv', 'cmf_60', \n",
    "    'vwma_60', 'vwma_120'\n",
    "]\n",
    "min_imputs = 9 # set len(dataset.columns) to disable inputs test\n",
    "outputs = 'Close_target'\n",
    "\n",
    "param_combinations = list(itertools.product(*param_grid.values()))\n",
    "input_combinations = [list(itertools.combinations(inputs, r)) for r in range(min_imputs, len(inputs) + 1)]\n",
    "input_combinations = [item for sublist in input_combinations for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bbe3b8",
   "metadata": {},
   "source": [
    "### 4. Entraînement du modèle GRU\n",
    "- Chargement de la bibliothèque `gru_lib` et initialisation du modèle GRU avec les paramètres définis.\n",
    "- Définition des hyperparamètres d'entraînement (`lr`, `batch_size`, `epochs`, etc.).\n",
    "- Prétraitement des données : découpage en ensembles d'entraînement, validation et test.\n",
    "- Lancement de l'entraînement et affichage des pertes (`Train Loss`, `Validation Loss`).\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Sauvegarde des résultats et du modèle\n",
    "- Création d'un dossier pour sauvegarder les résultats de l'entraînement.\n",
    "- Sauvegarde des métadonnées (`metada.json`), des pertes (`loss.csv`), des prédictions (`pred.csv`) et du modèle entraîné (`model.pth`).\n",
    "- Affichage des métriques d'évaluation (`R²`, `MAPE`, `MSE`, `RMSE`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "------------- Launch Time: 2025-01-26 23:18:42 ------------- Progress: 0/1 -------------\n",
      "Epoch 1/3, Train Loss: 90,193.467652, Validation Loss: 89,482.664866, Time: 0:00:01.577580\n",
      "Epoch 2/3, Train Loss: 89,481.785584, Validation Loss: 89,470.215303, Time: 0:00:01.582819\n",
      "Epoch 3/3, Train Loss: 89,471.027632, Validation Loss: 89,460.639213, Time: 0:00:01.606287\n",
      "\n",
      "ID: d0af0365-d30c-49ce-8dde-ae200baa7717\n",
      "Time elapsed: 4.7687\n",
      "Train Loss: 89,471.027632\n",
      "Val Loss: 89,460.639213\n",
      "Test Loss: 89,460.863042\n",
      "R^2 : 0.5503\n",
      "R^2 ajusté : 0.5476\n",
      "MAPE : 0.0064\n",
      "MSE : 827.954482\n",
      "RMSE : 28.774198\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import winsound\n",
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "import torch\n",
    "\n",
    "print(f'Cuda available: {torch.cuda.is_available()}')\n",
    "print(f'Cuda version: {torch.version.cuda}')\n",
    "print(f'Cuda device name: {torch.cuda.get_device_name(0)}')\n",
    "from gru_lib import GRUModel, Pipeline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "print('\\n')\n",
    "\n",
    "counter = 0\n",
    "for param_comb in param_combinations:\n",
    "    for input_comb in input_combinations:\n",
    "        time_lauch = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        print(f'------------- Launch Time: {time_lauch:<10} ------------- Progress: {(counter):,}/{(len(param_combinations)*len(input_combinations)):,} -------------')\n",
    "\n",
    "        # Extraire les paramètres pour cette combinaison\n",
    "        params = dict(zip(param_grid.keys(), param_comb))\n",
    "        inputs = list(input_comb)\n",
    "\n",
    "        # Garder la période étudiée\n",
    "        dataset_subset = dataset.iloc[-params[\"period_size\"]:]\n",
    "        dataset_subset = dataset_subset.dropna()\n",
    "\n",
    "        # Initialiser le modèle\n",
    "        model = GRUModel(\n",
    "            input_size=len(inputs),\n",
    "            output_size=1,\n",
    "            hidden_size=params[\"hidden_size\"], \n",
    "            num_layers=params[\"num_layers\"],\n",
    "            dropout=params[\"dropout\"],\n",
    "            device=params[\"device\"]\n",
    "        )\n",
    "\n",
    "        # Initialiser le pipeline\n",
    "        pipeline = Pipeline(\n",
    "            model=model, \n",
    "            dataset=dataset_subset,\n",
    "            inputs=inputs,\n",
    "            outputs=outputs,\n",
    "            ticker=ticker,\n",
    "            freq=freq\n",
    "        )\n",
    "\n",
    "        # Définir les hyperparamètres\n",
    "        pipeline.hyper_param(\n",
    "            lr=params[\"lr\"], \n",
    "            batch_size=params[\"batch_size\"], \n",
    "            epochs=params[\"epochs\"],\n",
    "            num_workers=params[\"num_workers\"],\n",
    "            delta=params[\"delta\"],\n",
    "            weight_decay=params[\"weight_decay\"]\n",
    "        )\n",
    "\n",
    "        # Prétraiter des données\n",
    "        pipeline.preprocess(\n",
    "            train_size=params[\"train_size\"],\n",
    "            val_size=params[\"val_size\"], \n",
    "            test_size=params[\"test_size\"], \n",
    "            seq_size=params[\"seq_size\"]\n",
    "        )\n",
    "\n",
    "        # Entraîner le modèle et évaluer les résultats\n",
    "        pipeline.train()\n",
    "        print(f\"\\nID: {str(pipeline.id)}\")\n",
    "        pipeline.eval()\n",
    "\n",
    "        # create folder and files\n",
    "        folder_path = os.getcwd() + '\\\\model\\\\' + str(pipeline.id)\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "        metadata = pipeline.metadata()\n",
    "        with open(folder_path + '\\\\metada.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(metadata, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        loss = pipeline.loss()\n",
    "        loss.to_csv(folder_path + '\\\\loss.csv', index=False)\n",
    "\n",
    "        pred = pipeline.pred()\n",
    "        pred.to_csv(folder_path + '\\\\pred.csv', index=False)\n",
    "\n",
    "        torch.save(pipeline.model, folder_path + '\\\\model.pth') # revoir l'optimisation --> sauvegarder les poids et le modèle séparément\n",
    "\n",
    "        print(\"\\n\")\n",
    "        counter += 1\n",
    "\n",
    "winsound.PlaySound(\"SystemExit\", winsound.SND_ALIAS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
