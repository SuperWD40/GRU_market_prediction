{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Cuda available: True\n",
      "Cuda version: 12.6\n",
      "Cuda device name: NVIDIA GeForce RTX 4060\n"
     ]
    }
   ],
   "source": [
    "import tech_lib as tech\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import winsound\n",
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "import torch\n",
    "print(f'Cuda available: {torch.cuda.is_available()}')\n",
    "print(f'Cuda version: {torch.version.cuda}')\n",
    "print(f'Cuda device name: {torch.cuda.get_device_name(0)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "ticker = 'BTCUSDT'\n",
    "freq = '1m'\n",
    "start_date_str = datetime(*(2024, 1, 15, 0, 0, 0)).strftime('%Y%m%d')\n",
    "end_date_str = datetime(*(2025, 1, 15, 0, 0, 0)).strftime('%Y%m%d')\n",
    "\n",
    "dataset = pd.read_csv(os.getcwd()+f'\\\\dataset\\\\\\{ticker}-{freq}-{start_date_str}-{end_date_str}.csv', index_col='Open Time')\n",
    "\n",
    "#Calcul des outputs\n",
    "dataset['Close_target'] = dataset['Close'].shift(-1)\n",
    "dataset['DirVar_target'] = pd.Series([1 if diff > 0 else 0 for diff in dataset['Close'].diff(1)], index=dataset.index, dtype=int).shift(-1)\n",
    "\n",
    "# Boucle pour calculer les indicateurs avec différentes fenêtres\n",
    "window = 60\n",
    "dataset[f'cmf_{window}'] = tech.cmf(\n",
    "    high=dataset.High, \n",
    "    low=dataset.Low, \n",
    "    close=dataset.Close, \n",
    "    volume=dataset.Volume, \n",
    "    window=window\n",
    ")\n",
    "\n",
    "for window in [60, 120]:\n",
    "    dataset[f'vwma_{window}'] = tech.vwma(\n",
    "        close=dataset.Close, \n",
    "        volume=dataset.Volume, \n",
    "        window=window\n",
    "    )\n",
    "    \n",
    "dataset['obv'] = tech.obv(dataset.Close, dataset.Volume)\n",
    "\n",
    "dataset = dataset.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"period_size\"   : [10000],\n",
    "    \"hidden_size\"   : [8],\n",
    "    \"num_layers\"    : [2],\n",
    "    \"dropout\"       : [0.15],\n",
    "    \"lr\"            : [0.001],\n",
    "    \"batch_size\"    : [128],\n",
    "    \"epochs\"        : [3],\n",
    "    \"num_workers\"   : [4],\n",
    "    \"delta\"         : [1],\n",
    "    \"weight_decay\"  : [1e-4],\n",
    "    \"train_size\"    : [0.7],\n",
    "    \"val_size\"      : [0.15],\n",
    "    \"test_size\"     : [0.15],\n",
    "    \"seq_size\"      : [60]\n",
    "}\n",
    "\n",
    "inputs = [\n",
    "    'High', 'Low', 'Open', 'Close',\n",
    "    'Volume', 'obv', 'cmf_60', \n",
    "    'vwma_60', 'vwma_120'\n",
    "]\n",
    "min_imputs = 9 # set len(dataset.columns) to disable inputs test\n",
    "outputs = 'Close_target'\n",
    "\n",
    "param_combinations = list(itertools.product(*param_grid.values()))\n",
    "input_combinations = [list(itertools.combinations(inputs, r)) for r in range(min_imputs, len(inputs) + 1)]\n",
    "input_combinations = [item for sublist in input_combinations for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "------------- Launch Time: 2025-01-26 23:18:42 ------------- Progress: 0/1 -------------\n",
      "Epoch 1/3, Train Loss: 90,193.467652, Validation Loss: 89,482.664866, Time: 0:00:01.577580\n",
      "Epoch 2/3, Train Loss: 89,481.785584, Validation Loss: 89,470.215303, Time: 0:00:01.582819\n",
      "Epoch 3/3, Train Loss: 89,471.027632, Validation Loss: 89,460.639213, Time: 0:00:01.606287\n",
      "\n",
      "ID: d0af0365-d30c-49ce-8dde-ae200baa7717\n",
      "Time elapsed: 4.7687\n",
      "Train Loss: 89,471.027632\n",
      "Val Loss: 89,460.639213\n",
      "Test Loss: 89,460.863042\n",
      "R^2 : 0.5503\n",
      "R^2 ajusté : 0.5476\n",
      "MAPE : 0.0064\n",
      "MSE : 827.954482\n",
      "RMSE : 28.774198\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from gru_lib import GRUModel, Pipeline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "counter = 0\n",
    "for param_comb in param_combinations:\n",
    "    for input_comb in input_combinations:\n",
    "        time_lauch = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        print(f'------------- Launch Time: {time_lauch:<10} ------------- Progress: {(counter):,}/{(len(param_combinations)*len(input_combinations)):,} -------------')\n",
    "\n",
    "        # Extraire les paramètres pour cette combinaison\n",
    "        params = dict(zip(param_grid.keys(), param_comb))\n",
    "        inputs = list(input_comb)\n",
    "\n",
    "        # Garder la période étudiée\n",
    "        dataset_subset = dataset.iloc[-params[\"period_size\"]:]\n",
    "        dataset_subset = dataset_subset.dropna()\n",
    "\n",
    "        # Initialiser le modèle\n",
    "        model = GRUModel(\n",
    "            input_size=len(inputs),\n",
    "            output_size=1,\n",
    "            hidden_size=params[\"hidden_size\"], \n",
    "            num_layers=params[\"num_layers\"],\n",
    "            dropout=params[\"dropout\"]\n",
    "        )\n",
    "\n",
    "        # Initialiser le pipeline\n",
    "        pipeline = Pipeline(\n",
    "            model=model, \n",
    "            dataset=dataset_subset,\n",
    "            inputs=inputs,\n",
    "            outputs=outputs,\n",
    "            ticker=ticker,\n",
    "            freq=freq\n",
    "        )\n",
    "\n",
    "        # Définir les hyperparamètres\n",
    "        pipeline.hyper_param(\n",
    "            lr=params[\"lr\"], \n",
    "            batch_size=params[\"batch_size\"], \n",
    "            epochs=params[\"epochs\"],\n",
    "            num_workers=params[\"num_workers\"],\n",
    "            delta=params[\"delta\"],\n",
    "            weight_decay=params[\"weight_decay\"]\n",
    "        )\n",
    "\n",
    "        # Prétraiter des données\n",
    "        pipeline.preprocess(\n",
    "            train_size=params[\"train_size\"],\n",
    "            val_size=params[\"val_size\"], \n",
    "            test_size=params[\"test_size\"], \n",
    "            seq_size=params[\"seq_size\"]\n",
    "        )\n",
    "\n",
    "        # Entraîner le modèle et évaluer les résultats\n",
    "        pipeline.train()\n",
    "        print(f\"\\nID: {str(pipeline.id)}\")\n",
    "        pipeline.eval()\n",
    "\n",
    "        # create folder and files\n",
    "        folder_path = os.getcwd() + '\\\\model\\\\' + str(pipeline.id)\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "        metadata = pipeline.metadata()\n",
    "        with open(folder_path + '\\\\metada.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(metadata, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        loss = pipeline.loss()\n",
    "        loss.to_csv(folder_path + '\\\\loss.csv', index=False)\n",
    "\n",
    "        pred = pipeline.pred()\n",
    "        pred.to_csv(folder_path + '\\\\pred.csv', index=False)\n",
    "\n",
    "        torch.save(pipeline.model, folder_path + '\\\\model.pth') # revoir l'optimisation --> sauvegarder les poids et le modèle séparément\n",
    "\n",
    "        print(\"\\n\")\n",
    "        counter += 1\n",
    "\n",
    "winsound.PlaySound(\"SystemExit\", winsound.SND_ALIAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(pipeline.model, folder_path + '\\\\model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
